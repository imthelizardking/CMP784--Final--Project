{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcYZ0EHImmrhkzOSzcSMVA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imthelizardking/CMP784--Final--Project/blob/main/suggestion_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import and install libraries and dependencies"
      ],
      "metadata": {
        "id": "Hp1uQlJFSml6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import collections\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import pylab\n",
        "use_colab = True # belki silinecek\n",
        "TINY = 1e-30\n",
        "EPS = 1e-4\n",
        "nax = np.newaxis"
      ],
      "metadata": {
        "id": "RitIBZ7jSmTz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to gdrive\n",
        "(Dataseti gdrive'dan çekiyordum, datalar local makineden çekileceği için bu kısım iptal edilebilir)"
      ],
      "metadata": {
        "id": "80JpNMo_SEpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeqcX2kmSEd1",
        "outputId": "64b5ce68-8969-40e1-ea58-662dd2f74519"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import data (Yine bu kısma da adres olarak dataset'in local makinedeki adresi verilmeli)"
      ],
      "metadata": {
        "id": "q5KZXzdeR_ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import data, from gdrive\n",
        "dirJson = \"/content/gdrive/MyDrive/cmp784_project_literature/colab repo/test_inputs/\"\n",
        "%cd \"/content/gdrive/MyDrive/cmp784_project_literature/colab repo/test_inputs\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWV0AylKR_Mc",
        "outputId": "0db01a2f-dcfa-47e9-9199-0e6858189a27"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/cmp784_project_literature/colab repo/test_inputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Burada, suggestion model trainer'a uygun matrisler üretiliyor."
      ],
      "metadata": {
        "id": "pRJT7x5QUVq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# labels_vector_ids: Bir image/annotation pair'ındaki itemler vektörün içerisine id'leriyle yazılıyor,\n",
        "# eğer pair'daki item'lar \"shorts\": (id=6) ve \"long sleeve top\" (id=1) ise, matrisin ilgili satırı [6, 1, 0, 0, ..., 0] oluyor\n",
        "# labels_vector_ids_size: her bir pair'da kaç tane item olduğunun bilgisi, yukarıdaki örnek için bu değer 2,\n",
        "json_files = [pos_json for pos_json in os.listdir(dirJson) if pos_json.endswith('.json')] # get all .json anno. files from dir\n",
        "NUM_FEATURES = 13 # number of features (types, color, style etc.)\n",
        "labels_vector_ids = np.zeros((len(json_files),NUM_FEATURES))\n",
        "labels_vector_ids_size = np.zeros(len(json_files))\n",
        "counter_outer = 0 # anno. counter\n",
        "typeDict = { # dictionary'imiz, deepfashion2'nun annotationlarındaki id'lere göre\n",
        "    \"short sleeve top\": 0,\n",
        "    \"long sleeve top\": 1,\n",
        "    \"short sleeve outwear\": 2,\n",
        "    \"long sleeve outwear\": 3,\n",
        "    \"vest\": 4,\n",
        "    \"sling\": 5,\n",
        "    \"shorts\": 6,\n",
        "    \"trousers\": 7,\n",
        "    \"skirt\": 8,\n",
        "    \"short sleeve dress\": 9,\n",
        "    \"long sleeve dress\": 10,\n",
        "    \"vest dress\": 11,\n",
        "    \"sling dress\": 12,\n",
        "}\n",
        "for json_ in json_files:\n",
        "  jsonFile = open(json_, 'r')\n",
        "  values = json.load(jsonFile)\n",
        "  jsonFile.close()\n",
        "  counter_inner = 0\n",
        "  for (k,v) in values.items():\n",
        "    if k.startswith('item'):    \n",
        "      labels_vector_ids[counter_outer][counter_inner] = typeDict[v[\"category_name\"]]\n",
        "      counter_inner = counter_inner + 1\n",
        "  labels_vector_ids_size[counter_outer] = counter_inner\n",
        "  counter_outer = counter_outer + 1\n",
        "print(\"labels_vector_ids_size: \", labels_vector_ids_size)\n",
        "#labels_vector_ids = labels_vector_ids + 1 # hepsine +1 yapmak gerekebilir, 1 olsun min\n",
        "print(\"labels_vector_ids: \", labels_vector_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_nNlqjvcv0N",
        "outputId": "7e3df58c-4c1c-43e7-c04a-8a4d5076525f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels_vector_ids_size:  [2. 1. 2. 2. 2. 1. 1. 1. 1. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 1. 1. 1. 1.\n",
            " 1. 2. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 1. 1. 2. 1. 2. 1. 2. 2.\n",
            " 2. 1. 2. 3. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1.\n",
            " 2. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 2. 2. 2. 1. 1. 1. 1. 1.\n",
            " 1. 1. 2. 1. 2. 1. 2. 2. 2. 1. 1. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 1. 1. 2.\n",
            " 1. 2. 2. 1. 2. 1. 1. 1. 1. 2. 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 2. 1. 2. 1. 1. 2. 2. 1. 2.\n",
            " 2. 2. 2. 2. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 2.\n",
            " 1. 2. 1. 1. 1. 1. 2. 1.]\n",
            "labels_vector_ids:  [[8. 0. 0. ... 0. 0. 0.]\n",
            " [4. 0. 0. ... 0. 0. 0.]\n",
            " [6. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [3. 0. 0. ... 0. 0. 0.]\n",
            " [7. 3. 0. ... 0. 0. 0.]\n",
            " [9. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compatility matrix (modified Co-occurrene matrix):"
      ],
      "metadata": {
        "id": "laVThcwonfrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Burada, co-occurence matrisini hesaplayıp, daha sonra bu matrise heart ve comment rating'lerini ekleyerek logaritmasını alıyoruz,\n",
        "# çıktı log_compatibility_matrix oluyor\n",
        "def calculate_log_co_occurence(labels_vector_ids, labels_vector_ids_size): # rating comment ve star eklenecek\n",
        "  dataset_size = len(labels_vector_ids)\n",
        "  scaler = 3\n",
        "  rating_heart = scaler * np.random.random_sample((13, 13, 13)) # heart rating ve comment rating elimde olmadığı için bunları random generate ettim,                                                                                            \n",
        "  rating_comment = scaler * np.random.random_sample((13, 13, 13)) # bunlar elimize geçince bu method'a input olarak verebiliriz.\n",
        "  log_co_occurence = np.zeros((13, 13, 13)) \n",
        "  for counter_temp in range(len(labels_vector_ids_size)):\n",
        "    i,j,k = 0,0,0\n",
        "    i,j,k = labels_vector_ids[counter_temp][0], labels_vector_ids[counter_temp][1], labels_vector_ids[counter_temp][2]\n",
        "    log_co_occurence[int(i)][int(j)][int(k)] +=1\n",
        "    # add ratings assignment with scaler/normalizer\n",
        "  \n",
        "  # ratingleri + olarak mı * olarak da düşünebiliriz, benim gdrive'dan çekebildiğim dataset çok küçük olduğu için + dedim şimdilik, ama *'ya çekebiliriz büyük datasette\n",
        "  #log_co_occurence = np.multiply(np.multiply(log_co_occurence, rating_heart), rating_comment)\n",
        "  log_co_occurence = log_co_occurence + rating_heart + rating_comment\n",
        "  delta_smoothing = 0.5  # A hyperparameter.  You can play with this if you want.\n",
        "  log_co_occurence += delta_smoothing  # Add delta so log doesn't break on 0's.\n",
        "  # maybe normalize\n",
        "  log_compatibility_matrix = np.log(log_co_occurence)\n",
        "  return log_compatibility_matrix"
      ],
      "metadata": {
        "id": "rk6zwwtenh_E"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data into two and create compatibility matrix for train and valid data"
      ],
      "metadata": {
        "id": "2SyMDSrrXmtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[data_train, data_valid] =  np.array_split(labels_vector_ids,2)\n",
        "[data_train_size, data_valid_size] = np.array_split(labels_vector_ids_size,2)\n",
        "log_co_occurence_train = calculate_log_co_occurence(data_train,data_train_size)\n",
        "print(log_co_occurence_train.shape)\n",
        "print(\"break\")\n",
        "log_co_occurence_valid = calculate_log_co_occurence(data_valid, data_valid_size)\n",
        "#print(log_co_occurence_valid)"
      ],
      "metadata": {
        "id": "gJkt6TG1vvxi",
        "outputId": "5a446798-07e0-45d4-c1af-546b885936cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13, 13, 13)\n",
            "break\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "New appr."
      ],
      "metadata": {
        "id": "56wXBjHv1tT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train helpers:"
      ],
      "metadata": {
        "id": "By1anVvEwpJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_GLoVE(W, b, log_co_occurence):\n",
        "  \"Compute the GLoVE loss.\"\n",
        "  n,_,_ = log_co_occurence.shape\n",
        "  _,_,emb_dim = W.shape\n",
        "  W_t = np.transpose(W,(0,emb_dim-1,1))\n",
        "  b_t = np.transpose(b,(0,emb_dim-1,1))\n",
        "  return np.sum((W @ W_t + b @ np.ones([1,n]) + np.ones([n,1])@b_t - log_co_occurence)**2)\n",
        "  \n",
        "def grad_GLoVE(W,  b, log_co_occurence):\n",
        "  \"Return the gradient of GLoVE objective w.r.t W and b.\"\n",
        "  \"INPUT: W - Vxd; b - Vx1; log_co_occurence: VxV\"\n",
        "  \"OUTPUT: grad_W - Vxd; grad_b - Vx1\"\n",
        "  # INPUT: W- 13x13xd; b - 13x13x1 log_comp: 13X13X13\n",
        "  # OUTPUT: grad_W - 13x13x1 grad_b: 13x13x1\n",
        "  n,_,_ = log_co_occurence.shape # n = #of item types, 13\n",
        "  _,_,emb_dim = W.shape\n",
        "  ###########################   YOUR CODE HERE  ##############################\n",
        "  print(W.shape)\n",
        "  W_t = np.transpose(W,(0,emb_dim-1,1))\n",
        "  b_t = np.transpose(b,(0,emb_dim-1,1))\n",
        "  log_t = np.transpose(log_co_occurence,(0,emb_dim-1,1))\n",
        "  \n",
        "  grad_W = 2*(W @ W_t + b @ np.ones([1,n]) + np.ones([n,1])@b_t - log_co_occurence) @ W\n",
        "  print(log_co_occurence.shape)\n",
        "  #grad_b = 2*(np.ones([1,100]) @ (W @ W_t + b @ np.ones([1,n]) + np.ones([n,1])@b_t - log_t))\n",
        "  #grad_b = np.transpose(grad_b,emb_dim,1)\n",
        "  grad_b = np.zeros((13,13,1))\n",
        "  ############################################################################\n",
        "  return grad_W, grad_b\n",
        "\n",
        "def train_GLoVE(W, b, log_co_occurence_train, log_co_occurence_valid, n_epochs, do_print=False):\n",
        "  #W:13x13xd b:13x13x1\n",
        "  n,_,_ = log_co_occurence_train.shape\n",
        "  learning_rate = 0.2 / n  # A hyperparameter.  You can play with this if you want.\n",
        "  for epoch in range(n_epochs):\n",
        "    grad_W, grad_b = grad_GLoVE(W, b, log_co_occurence_train)\n",
        "    W -= learning_rate * grad_W\n",
        "    b -= learning_rate * grad_b\n",
        "    train_loss, valid_loss = loss_GLoVE(W, b, log_co_occurence_train), loss_GLoVE(W, b, log_co_occurence_valid)\n",
        "    if do_print:\n",
        "      print(f\"Train Loss: {train_loss}, valid loss: {valid_loss}, grad_norm: {np.sum(grad_W**2)}\")\n",
        "  return W, b, train_loss, valid_loss"
      ],
      "metadata": {
        "id": "Bfwy5VYkwqV4"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainer:"
      ],
      "metadata": {
        "id": "LXGMZBK8rRcO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Trainer Parameters and Train"
      ],
      "metadata": {
        "id": "RRfZWm6_R4sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "n_epochs = 10  # 500 A hyperparameter.  You can play with this if you want.\n",
        "embedding_dims = np.array([3])  # Play with this\n",
        "final_train_losses, final_val_losses = [], []  # Store the final losses for graphing\n",
        "W_final_2d, b_final_2d = None, None\n",
        "do_print = True  # If you want to see diagnostic information during training\n",
        "for embedding_dim in tqdm(embedding_dims):\n",
        "  init_variance = 0.5  # A hyperparameter.  You can play with this if you want.\n",
        "  W = init_variance * np.random.normal(size=(13, 13, embedding_dim))\n",
        "  b = init_variance * np.random.normal(size=(13, 13, 1))\n",
        "  if do_print:\n",
        "    print(f\"Training for embedding dimension: {embedding_dim}\")\n",
        "  W_final, b_final, train_loss, valid_loss = train_GLoVE(W, b, log_co_occurence_train, log_co_occurence_valid, n_epochs, do_print=False)\n",
        "  if embedding_dim == 2:\n",
        "    # Save a parameter copy if we are training 2d embedding for visualization later\n",
        "    W_final_2d = W_final\n",
        "    b_final_2d = b_final\n",
        "  final_train_losses += [train_loss]\n",
        "  final_val_losses += [valid_loss]\n",
        "  if do_print:\n",
        "    print(f\"Final validation loss: {valid_loss}\")"
      ],
      "metadata": {
        "id": "eocaVf89R6VA",
        "outputId": "41f5cc40-cc11-4783-bc45-21900345ef3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 243.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for embedding dimension: 3\n",
            "(13, 13, 3)\n",
            "(13, 13, 13)\n",
            "(13, 13, 3)\n",
            "(13, 13, 13)\n",
            "(13, 13, 3)\n",
            "(13, 13, 13)\n",
            "(13, 13, 3)\n",
            "(13, 13, 13)\n",
            "(13, 13, 3)\n",
            "(13, 13, 13)\n",
            "(13, 13, 3)\n",
            "(13, 13, 13)\n",
            "(13, 13, 3)\n",
            "(13, 13, 13)\n",
            "(13, 13, 3)\n",
            "(13, 13, 13)\n",
            "(13, 13, 3)\n",
            "(13, 13, 13)\n",
            "(13, 13, 3)\n",
            "(13, 13, 13)\n",
            "Final validation loss: 565.6582891768307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot training metrics:"
      ],
      "metadata": {
        "id": "2hL8xTSbxBvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pylab.loglog(embedding_dims, final_train_losses)\n",
        "pylab.xlabel(\"Embedding Dimension\")\n",
        "pylab.ylabel(\"Training Loss\")\n",
        "pylab.legend()"
      ],
      "metadata": {
        "id": "5n2nsXd6xDyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pylab.loglog(embedding_dims, final_val_losses)\n",
        "pylab.xlabel(\"Embedding Dimension\")\n",
        "pylab.ylabel(\"Validation Loss\")\n",
        "pylab.legend()"
      ],
      "metadata": {
        "id": "aLoWdeSlxF5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREDICTOR**"
      ],
      "metadata": {
        "id": "X1Nh0VWsvTEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process input"
      ],
      "metadata": {
        "id": "Lpw3BWi_Sw5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = np.array([1,0,0,0,0,0,0,0,0,0,0,0,0]) + 1\n",
        "test_input_processed = np.zeros(len(test_input))\n",
        "counter_distinct = 0\n",
        "for e in range(len(test_input)):\n",
        "  if e!=0:\n",
        "    test_input_processed[counter_distinct] = e\n",
        "    counter_distinct = counter_distinct + 1"
      ],
      "metadata": {
        "id": "D_VrppK2Sy4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Combinations of Input"
      ],
      "metadata": {
        "id": "HBb-zDWArkYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def createCombinations(test_input_processed, counter_distinct):\n",
        "  counter_array = 0\n",
        "  if counter_distinct==0 or counter_distinct==1:\n",
        "    print(\"Can't make any prediction for item size smaller than 2\")\n",
        "    return\n",
        "  else:\n",
        "    if counter_distinct==2:\n",
        "      test_array = np.zeros(2)\n",
        "      test_array[0] = test_array[0] + test_input_processed[0]\n",
        "      test_array[1] = test_array[1] + test_input_processed[1]\n",
        "    elif counter_distinct==3:\n",
        "      test_array = np.zeros(3)\n",
        "      test_array[0] = test_array[0] + test_input_processed[0]\n",
        "      test_array[1] = test_array[1] + test_input_processed[1]\n",
        "      test_array[2] = test_array[2] + test_input_processed[2]              "
      ],
      "metadata": {
        "id": "3WndRZ0hrmRV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictor:"
      ],
      "metadata": {
        "id": "DNXEnQfGvYsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predictor():\n",
        "  "
      ],
      "metadata": {
        "id": "Nc9RO_7_vZ-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Preditor for each combination"
      ],
      "metadata": {
        "id": "I4cTp1YOr3SP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sTtKF6WAr5WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose b/w most probable results"
      ],
      "metadata": {
        "id": "oLzmoiQ6r6fq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kkATJE06r_Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feed suggestion into language model"
      ],
      "metadata": {
        "id": "xg9i6wfXsCya"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZIYe0hc9sFsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output image, bb and textual"
      ],
      "metadata": {
        "id": "e3aVRviNsGEb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OuYsQpQPsKSS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}