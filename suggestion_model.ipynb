{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1+nMFWmhjuZuV5zS9w6U3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imthelizardking/CMP784--Final--Project/blob/main/suggestion_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import and install libraries, dependencies"
      ],
      "metadata": {
        "id": "Hp1uQlJFSml6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import collections\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import pylab\n",
        "use_colab = True # belki çıkacak\n",
        "TINY = 1e-30\n",
        "EPS = 1e-4\n",
        "nax = np.newaxis"
      ],
      "metadata": {
        "id": "RitIBZ7jSmTz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to gdrive"
      ],
      "metadata": {
        "id": "80JpNMo_SEpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeqcX2kmSEd1",
        "outputId": "9014cff0-1d5e-4a8c-c0c5-c3e7dd430d26"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import data"
      ],
      "metadata": {
        "id": "q5KZXzdeR_ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import data, from gdrive\n",
        "dirJson = \"/content/gdrive/MyDrive/cmp784_project_literature/colab repo/test_inputs/\"\n",
        "%cd \"/content/gdrive/MyDrive/cmp784_project_literature/colab repo/test_inputs\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWV0AylKR_Mc",
        "outputId": "505b2099-c534-42cb-e641-d0cd99fe3c51"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/cmp784_project_literature/colab repo/test_inputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_files = [pos_json for pos_json in os.listdir(dirJson) if pos_json.endswith('.json')] # get all .json anno. files from dir\n",
        "NUM_FEATURES = 13 # number of features (types, color, style etc.)\n",
        "labels_vector = np.zeros((len(json_files),NUM_FEATURES)) # one hot encode matrix\n",
        "labels_vector_ids = np.zeros((len(json_files),NUM_FEATURES))\n",
        "labels_vector_ids_size = np.zeros(len(json_files))\n",
        "counter_outer = 0 # anno. counter\n",
        "# dictionary of clothing types, will be expanded\n",
        "#typeDict ={\n",
        "#    \"vest\": 0, #sets index of type to 0, if item exist index0 will be 1\n",
        "#    \"sling dress\": 1,\n",
        "#    \"short sleeve dress\": 2\n",
        "#}\n",
        "typeDict = {\n",
        "    \"short sleeve top\": 0, # shifted -1 due to \n",
        "    \"long sleeve top\": 1,\n",
        "    \"short sleeve outwear\": 2,\n",
        "    \"long sleeve outwear\": 3,\n",
        "    \"vest\": 4,\n",
        "    \"sling\": 5,\n",
        "    \"shorts\": 6,\n",
        "    \"trousers\": 7,\n",
        "    \"skirt\": 8,\n",
        "    \"short sleeve dress\": 9,\n",
        "    \"long sleeve dres\": 10,\n",
        "    \"vest dress\": 11,\n",
        "    \"sling dress\": 12,\n",
        "}\n",
        "for json_ in json_files:\n",
        "  jsonFile = open(json_, 'r')\n",
        "  values = json.load(jsonFile)\n",
        "  jsonFile.close()\n",
        "  counter_inner = 0\n",
        "  for (k,v) in values.items():\n",
        "    if k.startswith('item'):    \n",
        "      labels_vector[counter_outer][typeDict[v[\"category_name\"]]] = 1\n",
        "      labels_vector_ids[counter_outer][counter_inner] = typeDict[v[\"category_name\"]]\n",
        "      counter_inner = counter_inner + 1\n",
        "  labels_vector_ids_size[counter_outer] = counter_inner\n",
        "  counter_outer = counter_outer + 1\n",
        "#print(labels_vector)\n",
        "print(labels_vector_ids_size)\n",
        "labels_vector_ids = labels_vector_ids + 1 # hepsine +1 yapmak gerekebilir, 1 olsun min\n",
        "print(labels_vector_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_nNlqjvcv0N",
        "outputId": "7b2e30d6-5ac6-4efd-b863-a4b4d7351c95"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 2. 1. 1. 1. 1. 1. 2. 1. 2. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1.\n",
            " 1. 1. 1. 1. 2. 1. 2.]\n",
            "[[ 9.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [12.  4.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 8.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 9.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 9.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 8.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 6.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 9.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 9.  7.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [12.  4.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  9.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [11.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 9.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 9.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [11.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 8.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 7.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 9.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [11.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 9.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 7.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [11.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 7. 11.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create co-occurence matrix:"
      ],
      "metadata": {
        "id": "laVThcwonfrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size = (len(json_files))\n",
        "def calculate_log_co_occurence(labels_vector_ids, labels_vector_ids_size, ratings_heart, ratings_comment):\n",
        "  \"Compute the log-co-occurence matrix for our data.\"\n",
        "  log_co_occurence = np.zeros((dataset_size, dataset_size, dataset_size)) # hepsi dataset_size ya da labels_vector_ids size olsun!!!\n",
        "\n",
        "  for label_vector in range(len(labels_vector_ids)):\n",
        "    i,j,k = 0,0,0\n",
        "    i,j,k = label_vector[0], label_vector[1], label_vector[2]\n",
        "    log_co_occurence[i][j][k] +=1\n",
        "    # add ratings assignment with scaler/normalizer\n",
        "\n",
        "  log_co_occurence = np.multiply(log_co_occurence, rating_matrix)\n",
        "  delta_smoothing = 0.5  # A hyperparameter.  You can play with this if you want.\n",
        "  log_co_occurence += delta_smoothing  # Add delta so log doesn't break on 0's.\n",
        "  # maybe normalize\n",
        "  log_compatibility_matrix = np.log(log_co_occurence)\n",
        "  return log_compatibility_matrix"
      ],
      "metadata": {
        "id": "rk6zwwtenh_E"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[data_train, data_valid] =  np.array_split(labels_vector_ids,2)\n",
        "log_compatibility_matrix_train = calculate_log_co_occurence(data_train)\n",
        "log_co_occurence_valid = calculate_log_co_occurence(data_valid)"
      ],
      "metadata": {
        "id": "gJkt6TG1vvxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train helpers:"
      ],
      "metadata": {
        "id": "By1anVvEwpJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_GLoVE(W, b, log_co_occurence):\n",
        "  \"Compute the GLoVE loss.\"\n",
        "  n,_ = log_co_occurence.shape\n",
        "  return np.sum((W @ W.T + b @ np.ones([1,n]) + np.ones([n,1])@b.T - log_co_occurence)**2)\n",
        "\n",
        "def grad_GLoVE(W,  b, log_co_occurence):\n",
        "  \"Return the gradient of GLoVE objective w.r.t W and b.\"\n",
        "  \"INPUT: W - Vxd; b - Vx1; log_co_occurence: VxV\"\n",
        "  \"OUTPUT: grad_W - Vxd; grad_b - Vx1\"\n",
        "  n,_ = log_co_occurence.shape\n",
        "  ###########################   YOUR CODE HERE  ##############################\n",
        "  grad_W = 2 * (W.T @ (W @ W.T + b @ np.ones([1,n]) + np.ones([n,1])@b.T - log_co_occurence).T).T\n",
        "  grad_b = 2 * (np.ones([1,250]) @ (W @ W.T + b @ np.ones([1,n]) + np.ones([n,1])@b.T - log_co_occurence).T).T\n",
        "  ############################################################################\n",
        "  return grad_W, grad_b"
      ],
      "metadata": {
        "id": "Bfwy5VYkwqV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainer:"
      ],
      "metadata": {
        "id": "LXGMZBK8rRcO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gqbrwzD3PkBj"
      },
      "outputs": [],
      "source": [
        "def train(W, b, log_co_occurence_train, log_co_occurence_valid, n_epochs, do_print=False):\n",
        "  \"Traing W and b according to GLoVE objective.\"\n",
        "  n,_ = log_co_occurence_train.shape\n",
        "  learning_rate = 0.5 / n  # 0.5 A hyperparameter.  You can play with this if you want.\n",
        "  for epoch in range(n_epochs):\n",
        "    grad_W, grad_b = grad_GLoVE(W, b, log_co_occurence_train)\n",
        "    W -= learning_rate * grad_W\n",
        "    b -= learning_rate * grad_b\n",
        "    train_loss, valid_loss = loss_GLoVE(W, b, log_co_occurence_train), loss_GLoVE(W, b, log_co_occurence_valid)\n",
        "    if do_print:\n",
        "      print(f\"Train Loss: {train_loss}, valid loss: {valid_loss}, grad_norm: {np.sum(grad_W**2)}\")\n",
        "  return W, b, train_loss, valid_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Trainer Parameters and Train"
      ],
      "metadata": {
        "id": "RRfZWm6_R4sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "n_epochs = 500  # 500 A hyperparameter.  You can play with this if you want.\n",
        "embedding_dims = np.array([2, 3, 4, 5, 6, 7, 8, 10, 12, 14, 16, 256])  # Play with this\n",
        "final_train_losses, final_val_losses = [], []  # Store the final losses for graphing\n",
        "W_final_2d, b_final_2d = None, None\n",
        "do_print = True  # If you want to see diagnostic information during training\n",
        "for embedding_dim in tqdm(embedding_dims):\n",
        "  init_variance = 0.5  # A hyperparameter.  You can play with this if you want.\n",
        "  W = init_variance * np.random.normal(size=(250, embedding_dim))\n",
        "  b = init_variance * np.random.normal(size=(250, 1))\n",
        "  if do_print:\n",
        "    print(f\"Training for embedding dimension: {embedding_dim}\")\n",
        "  W_final, b_final, train_loss, valid_loss = train_GLoVE(W, b, log_co_occurence_train, log_co_occurence_valid, n_epochs, do_print=False)\n",
        "  if embedding_dim == 2:\n",
        "    # Save a parameter copy if we are training 2d embedding for visualization later\n",
        "    W_final_2d = W_final\n",
        "    b_final_2d = b_final\n",
        "  final_train_losses += [train_loss]\n",
        "  final_val_losses += [valid_loss]\n",
        "  if do_print:\n",
        "    print(f\"Final validation loss: {valid_loss}\")"
      ],
      "metadata": {
        "id": "eocaVf89R6VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot training metrics:"
      ],
      "metadata": {
        "id": "2hL8xTSbxBvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pylab.loglog(embedding_dims, final_train_losses)\n",
        "pylab.xlabel(\"Embedding Dimension\")\n",
        "pylab.ylabel(\"Training Loss\")\n",
        "pylab.legend()"
      ],
      "metadata": {
        "id": "5n2nsXd6xDyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pylab.loglog(embedding_dims, final_val_losses)\n",
        "pylab.xlabel(\"Embedding Dimension\")\n",
        "pylab.ylabel(\"Validation Loss\")\n",
        "pylab.legend()"
      ],
      "metadata": {
        "id": "aLoWdeSlxF5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREDICTOR**"
      ],
      "metadata": {
        "id": "X1Nh0VWsvTEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process input"
      ],
      "metadata": {
        "id": "Lpw3BWi_Sw5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = np.array([1,0,0,0,0,0,0,0,0,0,0,0,0]) + 1\n",
        "test_input_processed = np.zeros(len(test_input))\n",
        "counter_distinct = 0\n",
        "for e in range(len(test_input)):\n",
        "  if e!=0:\n",
        "    test_input_processed[counter_distinct] = e\n",
        "    counter_distinct = counter_distinct + 1"
      ],
      "metadata": {
        "id": "D_VrppK2Sy4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Combinations of Input"
      ],
      "metadata": {
        "id": "HBb-zDWArkYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def createCombinations(test_input_processed, counter_distinct):\n",
        "  counter_array = 0\n",
        "  if counter_distinct==0 or counter_distinct==1:\n",
        "    print(\"Can't make any prediction for item size smaller than 2\")\n",
        "    return\n",
        "  else:\n",
        "    if counter_distinct==2:\n",
        "      test_array = np.zeros(2)\n",
        "      test_array[0] = test_array[0] + test_input_processed[0]\n",
        "      test_array[1] = test_array[1] + test_input_processed[1]\n",
        "    elif counter_distinct==3:\n",
        "      test_array = np.zeros(3)\n",
        "      test_array[0] = test_array[0] + test_input_processed[0]\n",
        "      test_array[1] = test_array[1] + test_input_processed[1]\n",
        "      test_array[2] = test_array[2] + test_input_processed[2]              "
      ],
      "metadata": {
        "id": "3WndRZ0hrmRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictor:"
      ],
      "metadata": {
        "id": "DNXEnQfGvYsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predictor():\n",
        "  "
      ],
      "metadata": {
        "id": "Nc9RO_7_vZ-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Preditor for each combination"
      ],
      "metadata": {
        "id": "I4cTp1YOr3SP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sTtKF6WAr5WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose b/w most probable results"
      ],
      "metadata": {
        "id": "oLzmoiQ6r6fq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kkATJE06r_Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feed suggestion into language model"
      ],
      "metadata": {
        "id": "xg9i6wfXsCya"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZIYe0hc9sFsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output image, bb and textual"
      ],
      "metadata": {
        "id": "e3aVRviNsGEb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OuYsQpQPsKSS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}